{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nTrain Your Own Neural Network Potential, Using PyTorch-Ignite\n=============================================================\n\nWe have seen how to train a neural network potential by manually writing\ntraining loop in `training-example`. TorchANI provide tools to work\nwith PyTorch-Ignite to simplify the writing of training code. This tutorial\nshows how to use these tools to train a demo model. The setup in this demo is\nnot necessarily identical to NeuroChem.\n\nThis tutorial assumes readers have read `training-example`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin with, let's first import the modules and setup devices we will use:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport ignite\nimport torchani\nimport timeit\nimport os\nimport ignite.contrib.handlers\nimport torch.utils.tensorboard\n\n# device to run the training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's setup training hyperparameters and dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# training and validation set\ntry:\n    path = os.path.dirname(os.path.realpath(__file__))\nexcept NameError:\n    path = os.getcwd()\ndspath = os.path.join(path, '../dataset/ani1-up_to_gdb4/ani_gdb_s01.h5')\n\n# checkpoint file to save model when validation RMSE improves\nmodel_checkpoint = 'model.pt'\n\n# max epochs to run the training\nmax_epochs = 20\n\n# Compute training RMSE every this steps. Since the training set is usually\n# huge and the loss funcition does not directly gives us RMSE, we need to\n# check the training RMSE to see overfitting.\ntraining_rmse_every = 5\n\n# batch size\nbatch_size = 2560\n\n# log directory for tensorboard\nlog = 'runs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead of manually specifying hyperparameters as in `training-example`,\nhere we will load them from files.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "const_file = os.path.join(path, '../torchani/resources/ani-1x_8x/rHCNO-5.2R_16-3.5A_a4-8.params')  # noqa: E501\nconsts = torchani.neurochem.Constants(const_file)\naev_computer = torchani.AEVComputer(**consts)\nenergy_shifter = torchani.utils.EnergyShifter(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's define atomic neural networks. Here in this demo, we use the same\nsize of neural network for all atom types, but this is not necessary.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def atomic():\n    model = torch.nn.Sequential(\n        torch.nn.Linear(384, 128),\n        torch.nn.CELU(0.1),\n        torch.nn.Linear(128, 128),\n        torch.nn.CELU(0.1),\n        torch.nn.Linear(128, 64),\n        torch.nn.CELU(0.1),\n        torch.nn.Linear(64, 1)\n    )\n    return model\n\n\nnn = torchani.ANIModel([atomic() for _ in range(4)])\nprint(nn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If checkpoint from previous training exists, then load it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if os.path.isfile(model_checkpoint):\n    nn.load_state_dict(torch.load(model_checkpoint))\nelse:\n    torch.save(nn.state_dict(), model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now create a pipeline of AEV Computer --> Neural Networks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = torchani.nn.Sequential(aev_computer, nn).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now setup tensorboard\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "writer = torch.utils.tensorboard.SummaryWriter(log_dir=log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now load training and validation datasets into memory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training, validation = torchani.data.load_ani_dataset(\n    dspath, consts.species_to_tensor, batch_size, rm_outlier=True, device=device,\n    transform=[energy_shifter.subtract_from_dataset], split=[0.8, None])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have tools to deal with the chunking (see `training-example`). These\ntools can be used as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "container = torchani.ignite.Container({'energies': model})\noptimizer = torch.optim.Adam(model.parameters())\ntrainer = ignite.engine.create_supervised_trainer(\n    container, optimizer, torchani.ignite.MSELoss('energies'))\nevaluator = ignite.engine.create_supervised_evaluator(\n    container,\n    metrics={\n        'RMSE': torchani.ignite.RMSEMetric('energies')\n    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's add a progress bar for the trainer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pbar = ignite.contrib.handlers.ProgressBar()\npbar.attach(trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And some event handlers to compute validation and training metrics:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def hartree2kcal(x):\n    return 627.509 * x\n\n\n@trainer.on(ignite.engine.Events.EPOCH_STARTED)\ndef validation_and_checkpoint(trainer):\n    def evaluate(dataset, name):\n        evaluator = ignite.engine.create_supervised_evaluator(\n            container,\n            metrics={\n                'RMSE': torchani.ignite.RMSEMetric('energies')\n            }\n        )\n        evaluator.run(dataset)\n        metrics = evaluator.state.metrics\n        rmse = hartree2kcal(metrics['RMSE'])\n        writer.add_scalar(name, rmse, trainer.state.epoch)\n\n    # compute validation RMSE\n    evaluate(validation, 'validation_rmse_vs_epoch')\n\n    # compute training RMSE\n    if trainer.state.epoch % training_rmse_every == 1:\n        evaluate(training, 'training_rmse_vs_epoch')\n\n    # checkpoint model\n    torch.save(nn.state_dict(), model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also some to log elapsed time:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n\n\n@trainer.on(ignite.engine.Events.EPOCH_STARTED)\ndef log_time(trainer):\n    elapsed = round(timeit.default_timer() - start, 2)\n    writer.add_scalar('time_vs_epoch', elapsed, trainer.state.epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also log the loss per iteration:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@trainer.on(ignite.engine.Events.ITERATION_COMPLETED)\ndef log_loss(trainer):\n    iteration = trainer.state.iteration\n    writer.add_scalar('loss_vs_iteration', trainer.state.output, iteration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally, we are ready to run:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.run(training, max_epochs)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}