{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nUse Disk Cache of AEV to Boost Training\n=======================================\n\nIn the previous `training-example` example, AEVs are computed everytime\nwhen needed. This is not very efficient because the AEVs actually never change\nduring training. If one has a good SSD, it would be beneficial to cache these\nAEVs.  This example shows how to use disk cache to boost training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most part of the codes in this example are line by line copy of\n`training-example`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport ignite\nimport torchani\nimport timeit\nimport os\nimport ignite.contrib.handlers\nimport torch.utils.tensorboard\n\n\n# training and validation set\ntry:\n    path = os.path.dirname(os.path.realpath(__file__))\nexcept NameError:\n    path = os.getcwd()\ntraining_path = os.path.join(path, '../dataset/ani1-up_to_gdb4/ani_gdb_s01.h5')\nvalidation_path = os.path.join(path, '../dataset/ani1-up_to_gdb4/ani_gdb_s01.h5')  # noqa: E501\n\n# checkpoint file to save model when validation RMSE improves\nmodel_checkpoint = 'model.pt'\n\n# max epochs to run the training\nmax_epochs = 20\n\n# Compute training RMSE every this steps. Since the training set is usually\n# huge and the loss funcition does not directly gives us RMSE, we need to\n# check the training RMSE to see overfitting.\ntraining_rmse_every = 5\n\n# device to run the training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# batch size\nbatch_size = 1024\n\n# log directory for tensorboardX\nlog = 'runs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, there is no need to manually construct aev computer and energy shifter,\nbut we do need to generate a disk cache for datasets\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "const_file = os.path.join(path, '../torchani/resources/ani-1x_8x/rHCNO-5.2R_16-3.5A_a4-8.params')\nsae_file = os.path.join(path, '../torchani/resources/ani-1x_8x/sae_linfit.dat')\ntraining_cache = './training_cache'\nvalidation_cache = './validation_cache'\n\n# If the cache dirs already exists, then we assume these data has already been\n# cached and skip the generation part.\nif not os.path.exists(training_cache):\n    torchani.data.cache_aev(training_cache, training_path, batch_size, device,\n                            const_file, True, sae_file)\nif not os.path.exists(validation_cache):\n    torchani.data.cache_aev(validation_cache, validation_path, batch_size,\n                            device, const_file, True, sae_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The codes that define the network are also the same\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def atomic():\n    model = torch.nn.Sequential(\n        torch.nn.Linear(384, 128),\n        torch.nn.CELU(0.1),\n        torch.nn.Linear(128, 128),\n        torch.nn.CELU(0.1),\n        torch.nn.Linear(128, 64),\n        torch.nn.CELU(0.1),\n        torch.nn.Linear(64, 1)\n    )\n    return model\n\n\nnn = torchani.ANIModel([atomic() for _ in range(4)])\nprint(nn)\n\nif os.path.isfile(model_checkpoint):\n    nn.load_state_dict(torch.load(model_checkpoint))\nelse:\n    torch.save(nn.state_dict(), model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Except that at here we do not include aev computer into our pipeline, because\nthe cache loader will load computed AEVs from disk.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = nn.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part is also a line by line copy\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "writer = torch.utils.tensorboard.SummaryWriter(log_dir=log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we don't need to construct :class:`torchani.data.BatchedANIDataset`\nobject, but instead an object of :class:`torchani.data.AEVCacheLoader`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training = torchani.data.AEVCacheLoader(training_cache)\nvalidation = torchani.data.AEVCacheLoader(validation_cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rest of the code are again the same\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "container = torchani.ignite.Container({'energies': model})\noptimizer = torch.optim.Adam(model.parameters())\ntrainer = ignite.engine.create_supervised_trainer(\n    container, optimizer, torchani.ignite.MSELoss('energies'))\nevaluator = ignite.engine.create_supervised_evaluator(\n    container,\n    metrics={\n        'RMSE': torchani.ignite.RMSEMetric('energies')\n    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's add a progress bar for the trainer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pbar = ignite.contrib.handlers.ProgressBar()\npbar.attach(trainer)\n\n\ndef hartree2kcal(x):\n    return 627.509 * x\n\n\n@trainer.on(ignite.engine.Events.EPOCH_STARTED)\ndef validation_and_checkpoint(trainer):\n    def evaluate(dataset, name):\n        evaluator = ignite.engine.create_supervised_evaluator(\n            container,\n            metrics={\n                'RMSE': torchani.ignite.RMSEMetric('energies')\n            }\n        )\n        evaluator.run(dataset)\n        metrics = evaluator.state.metrics\n        rmse = hartree2kcal(metrics['RMSE'])\n        writer.add_scalar(name, rmse, trainer.state.epoch)\n\n    # compute validation RMSE\n    evaluate(validation, 'validation_rmse_vs_epoch')\n\n    # compute training RMSE\n    if trainer.state.epoch % training_rmse_every == 1:\n        evaluate(training, 'training_rmse_vs_epoch')\n\n    # checkpoint model\n    torch.save(nn.state_dict(), model_checkpoint)\n\n\nstart = timeit.default_timer()\n\n\n@trainer.on(ignite.engine.Events.EPOCH_STARTED)\ndef log_time(trainer):\n    elapsed = round(timeit.default_timer() - start, 2)\n    writer.add_scalar('time_vs_epoch', elapsed, trainer.state.epoch)\n\n\n@trainer.on(ignite.engine.Events.ITERATION_COMPLETED)\ndef log_loss(trainer):\n    iteration = trainer.state.iteration\n    writer.add_scalar('loss_vs_iteration', trainer.state.output, iteration)\n\n\ntrainer.run(training, max_epochs)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}