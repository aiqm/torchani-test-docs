{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nTrain Neural Network Potential To Both Energies and Forces\n==========================================================\n\nWe have seen how to train a neural network potential by manually writing\ntraining loop in `training-example`. This tutorial shows how to modify\nthat script to train to force.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most part of the script are the same as `training-example`, we will omit\nthe comments for these parts. Please refer to `training-example` for more\ninformation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchani\nimport os\nimport math\nimport torch.utils.tensorboard\nimport tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nRcr = 5.2000e+00\nRca = 3.5000e+00\nEtaR = torch.tensor([1.6000000e+01], device=device)\nShfR = torch.tensor([9.0000000e-01, 1.1687500e+00, 1.4375000e+00, 1.7062500e+00, 1.9750000e+00, 2.2437500e+00, 2.5125000e+00, 2.7812500e+00, 3.0500000e+00, 3.3187500e+00, 3.5875000e+00, 3.8562500e+00, 4.1250000e+00, 4.3937500e+00, 4.6625000e+00, 4.9312500e+00], device=device)\nZeta = torch.tensor([3.2000000e+01], device=device)\nShfZ = torch.tensor([1.9634954e-01, 5.8904862e-01, 9.8174770e-01, 1.3744468e+00, 1.7671459e+00, 2.1598449e+00, 2.5525440e+00, 2.9452431e+00], device=device)\nEtaA = torch.tensor([8.0000000e+00], device=device)\nShfA = torch.tensor([9.0000000e-01, 1.5500000e+00, 2.2000000e+00, 2.8500000e+00], device=device)\nnum_species = 4\naev_computer = torchani.AEVComputer(Rcr, Rca, EtaR, ShfR, EtaA, Zeta, ShfA, ShfZ, num_species)\nenergy_shifter = torchani.utils.EnergyShifter(None)\nspecies_to_tensor = torchani.utils.ChemicalSymbolsToInts('HCNO')\n\n\ntry:\n    path = os.path.dirname(os.path.realpath(__file__))\nexcept NameError:\n    path = os.getcwd()\ndspath = os.path.join(path, '../dataset/ani-1x/sample.h5')\n\nbatch_size = 2560"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code to create the dataset is a bit different: we need to manually\nspecify that ``atomic_properties=['forces']`` so that forces will be read\nfrom hdf5 files.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training, validation = torchani.data.load_ani_dataset(\n    dspath, species_to_tensor, batch_size, rm_outlier=True,\n    device=device, atomic_properties=['forces'],\n    transform=[energy_shifter.subtract_from_dataset], split=[0.8, None])\n\nprint('Self atomic energies: ', energy_shifter.self_energies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When iterating the dataset, we will get pairs of input and output\n``(species_coordinates, properties)``, in this case, ``properties`` would\ncontain a key ``'atomic'`` where ``properties['atomic']`` is a list of dict\ncontaining forces:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = training[0]\nproperties = data[1]\natomic_properties = properties['atomic']\nprint(type(atomic_properties))\nprint(list(atomic_properties[0].keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to padding, part of the forces might be 0\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(atomic_properties[0]['forces'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code to define networks, optimizers, are mostly the same\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "H_network = torch.nn.Sequential(\n    torch.nn.Linear(384, 160),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(160, 128),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(128, 96),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(96, 1)\n)\n\nC_network = torch.nn.Sequential(\n    torch.nn.Linear(384, 144),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(144, 112),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(112, 96),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(96, 1)\n)\n\nN_network = torch.nn.Sequential(\n    torch.nn.Linear(384, 128),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(128, 112),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(112, 96),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(96, 1)\n)\n\nO_network = torch.nn.Sequential(\n    torch.nn.Linear(384, 128),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(128, 112),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(112, 96),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(96, 1)\n)\n\nnn = torchani.ANIModel([H_network, C_network, N_network, O_network])\nprint(nn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize the weights and biases.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Pytorch default initialization for the weights and biases in linear layers\n  is Kaiming uniform. See: `TORCH.NN.MODULES.LINEAR`_\n  We initialize the weights similarly but from the normal distribution.\n  The biases were initialized to zero.</p></div>\n\n  https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def init_params(m):\n    if isinstance(m, torch.nn.Linear):\n        torch.nn.init.kaiming_normal_(m.weight, a=1.0)\n        torch.nn.init.zeros_(m.bias)\n\n\nnn.apply(init_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now create a pipeline of AEV Computer --> Neural Networks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = torchani.nn.Sequential(aev_computer, nn).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will use Adam with weight decay for the weights and Stochastic Gradient\nDescent for biases.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "AdamW = torchani.optim.AdamW([\n    # H networks\n    {'params': [H_network[0].weight]},\n    {'params': [H_network[2].weight], 'weight_decay': 0.00001},\n    {'params': [H_network[4].weight], 'weight_decay': 0.000001},\n    {'params': [H_network[6].weight]},\n    # C networks\n    {'params': [C_network[0].weight]},\n    {'params': [C_network[2].weight], 'weight_decay': 0.00001},\n    {'params': [C_network[4].weight], 'weight_decay': 0.000001},\n    {'params': [C_network[6].weight]},\n    # N networks\n    {'params': [N_network[0].weight]},\n    {'params': [N_network[2].weight], 'weight_decay': 0.00001},\n    {'params': [N_network[4].weight], 'weight_decay': 0.000001},\n    {'params': [N_network[6].weight]},\n    # O networks\n    {'params': [O_network[0].weight]},\n    {'params': [O_network[2].weight], 'weight_decay': 0.00001},\n    {'params': [O_network[4].weight], 'weight_decay': 0.000001},\n    {'params': [O_network[6].weight]},\n])\n\nSGD = torch.optim.SGD([\n    # H networks\n    {'params': [H_network[0].bias]},\n    {'params': [H_network[2].bias]},\n    {'params': [H_network[4].bias]},\n    {'params': [H_network[6].bias]},\n    # C networks\n    {'params': [C_network[0].bias]},\n    {'params': [C_network[2].bias]},\n    {'params': [C_network[4].bias]},\n    {'params': [C_network[6].bias]},\n    # N networks\n    {'params': [N_network[0].bias]},\n    {'params': [N_network[2].bias]},\n    {'params': [N_network[4].bias]},\n    {'params': [N_network[6].bias]},\n    # O networks\n    {'params': [O_network[0].bias]},\n    {'params': [O_network[2].bias]},\n    {'params': [O_network[4].bias]},\n    {'params': [O_network[6].bias]},\n], lr=1e-3)\n\nAdamW_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(AdamW, factor=0.5, patience=100, threshold=0)\nSGD_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(SGD, factor=0.5, patience=100, threshold=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part of the code is also the same\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "latest_checkpoint = 'force-training-latest.pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resume training from previously saved checkpoints:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if os.path.isfile(latest_checkpoint):\n    checkpoint = torch.load(latest_checkpoint)\n    nn.load_state_dict(checkpoint['nn'])\n    AdamW.load_state_dict(checkpoint['AdamW'])\n    SGD.load_state_dict(checkpoint['SGD'])\n    AdamW_scheduler.load_state_dict(checkpoint['AdamW_scheduler'])\n    SGD_scheduler.load_state_dict(checkpoint['SGD_scheduler'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During training, we need to validate on validation set and if validation error\nis better than the best, then save the new best model to a checkpoint\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# helper function to convert energy unit from Hartree to kcal/mol\ndef hartree2kcal(x):\n    return 627.509 * x\n\n\ndef validate():\n    # run validation\n    mse_sum = torch.nn.MSELoss(reduction='sum')\n    total_mse = 0.0\n    count = 0\n    for batch_x, batch_y in validation:\n        true_energies = batch_y['energies']\n        predicted_energies = []\n        for chunk_species, chunk_coordinates in batch_x:\n            _, chunk_energies = model((chunk_species, chunk_coordinates))\n            predicted_energies.append(chunk_energies)\n        predicted_energies = torch.cat(predicted_energies)\n        total_mse += mse_sum(predicted_energies, true_energies).item()\n        count += predicted_energies.shape[0]\n    return hartree2kcal(math.sqrt(total_mse / count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will also use TensorBoard to visualize our training process\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensorboard = torch.utils.tensorboard.SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the training loop, we need to compute force, and loss for forces\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mse = torch.nn.MSELoss(reduction='none')\n\nprint(\"training starting from epoch\", AdamW_scheduler.last_epoch + 1)\n# We only train 3 epoches here in able to generate the docs quickly.\n# Real training should take much more than 3 epoches.\nmax_epochs = 3\nearly_stopping_learning_rate = 1.0E-5\nforce_coefficient = 0.1  # controls the importance of energy loss vs force loss\nbest_model_checkpoint = 'force-training-best.pt'\n\nfor _ in range(AdamW_scheduler.last_epoch + 1, max_epochs):\n    rmse = validate()\n    print('RMSE:', rmse, 'at epoch', AdamW_scheduler.last_epoch + 1)\n\n    learning_rate = AdamW.param_groups[0]['lr']\n\n    if learning_rate < early_stopping_learning_rate:\n        break\n\n    # checkpoint\n    if AdamW_scheduler.is_better(rmse, AdamW_scheduler.best):\n        torch.save(nn.state_dict(), best_model_checkpoint)\n\n    AdamW_scheduler.step(rmse)\n    SGD_scheduler.step(rmse)\n\n    tensorboard.add_scalar('validation_rmse', rmse, AdamW_scheduler.last_epoch)\n    tensorboard.add_scalar('best_validation_rmse', AdamW_scheduler.best, AdamW_scheduler.last_epoch)\n    tensorboard.add_scalar('learning_rate', learning_rate, AdamW_scheduler.last_epoch)\n\n    # Besides being stored in x, species and coordinates are also stored in y.\n    # So here, for simplicity, we just ignore the x and use y for everything.\n    for i, (_, batch_y) in tqdm.tqdm(\n        enumerate(training),\n        total=len(training),\n        desc=\"epoch {}\".format(AdamW_scheduler.last_epoch)\n    ):\n\n        true_energies = batch_y['energies']\n        predicted_energies = []\n        num_atoms = []\n        force_loss = []\n\n        for chunk in batch_y['atomic']:\n            chunk_species = chunk['species']\n            chunk_coordinates = chunk['coordinates']\n            chunk_true_forces = chunk['forces']\n            chunk_num_atoms = (chunk_species >= 0).to(true_energies.dtype).sum(dim=1)\n            num_atoms.append(chunk_num_atoms)\n\n            # We must set `chunk_coordinates` to make it requires grad, so\n            # that we could compute force from it\n            chunk_coordinates.requires_grad_(True)\n\n            _, chunk_energies = model((chunk_species, chunk_coordinates))\n\n            # We can use torch.autograd.grad to compute force. Remember to\n            # create graph so that the loss of the force can contribute to\n            # the gradient of parameters, and also to retain graph so that\n            # we can backward through it a second time when computing gradient\n            # w.r.t. parameters.\n            chunk_forces = -torch.autograd.grad(chunk_energies.sum(), chunk_coordinates, create_graph=True, retain_graph=True)[0]\n\n            # Now let's compute loss for force of this chunk\n            chunk_force_loss = mse(chunk_true_forces, chunk_forces).sum(dim=(1, 2)) / chunk_num_atoms\n\n            predicted_energies.append(chunk_energies)\n            force_loss.append(chunk_force_loss)\n\n        num_atoms = torch.cat(num_atoms)\n        predicted_energies = torch.cat(predicted_energies)\n\n        # Now the total loss has two parts, energy loss and force loss\n        energy_loss = (mse(predicted_energies, true_energies) / num_atoms.sqrt()).mean()\n        force_loss = torch.cat(force_loss).mean()\n        loss = energy_loss + force_coefficient * force_loss\n\n        AdamW.zero_grad()\n        SGD.zero_grad()\n        loss.backward()\n        AdamW.step()\n        SGD.step()\n\n        # write current batch loss to TensorBoard\n        tensorboard.add_scalar('batch_loss', loss, AdamW_scheduler.last_epoch * len(training) + i)\n\n    torch.save({\n        'nn': nn.state_dict(),\n        'AdamW': AdamW.state_dict(),\n        'SGD': SGD.state_dict(),\n        'AdamW_scheduler': AdamW_scheduler.state_dict(),\n        'SGD_scheduler': SGD_scheduler.state_dict(),\n    }, latest_checkpoint)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}